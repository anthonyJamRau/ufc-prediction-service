{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rautm\\Documents\\Notebooks\\UFC Project\\Scraper\\fights.py:86: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  return \"Women's \" + re.findall('\\w*weight',fight_type[0].text.strip())[0]\n",
      "c:\\Users\\rautm\\Documents\\Notebooks\\UFC Project\\Scraper\\fights.py:96: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  return re.findall('\\w*weight',fight_type[0].text.strip())[0]\n",
      "c:\\Users\\rautm\\Documents\\Notebooks\\UFC Project\\Scraper\\fights.py:164: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  finish_time = re.findall('\\d:\\d\\d',overview[1].text)[0]\n"
     ]
    }
   ],
   "source": [
    "from Scraper import get_urls, events, fights, fightstats, fighters, upcoming_events, normalise_tables\n",
    "def main():\n",
    "\n",
    "#     #Scrapes all urls from ufcstats.com\n",
    "#     get_urls.get_event_urls()\n",
    "#     get_urls.get_fight_urls()\n",
    "#     get_urls.get_fighter_urls()\n",
    "\n",
    "#     #Iterates through urls and scrapes key data into csv files\n",
    "    events.scrape_events()\n",
    "    # fights.scrape_fights()\n",
    "    # fightstats.scrape_fightstats()\n",
    "    # fighters.scrape_fighters()\n",
    "    # upcoming_events.scrape_upcoming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping event links from ufcstats.com\n"
     ]
    },
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_urls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_event_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# get_urls.get_fight_urls()\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# get_urls.get_fighter_urls()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rautm\\Documents\\Notebooks\\UFC Project\\Scraper\\get_urls.py:26\u001b[0m, in \u001b[0;36mget_event_urls\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScraping event links from ufcstats.com\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m main_url \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://ufcstats.com/statistics/events/completed?page=all\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m main_event_soup \u001b[38;5;241m=\u001b[39m \u001b[43mbs4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_url\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#Adds href to list if href contains a link with keyword 'event-details'\u001b[39;00m\n\u001b[0;32m     29\u001b[0m all_event_urls \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m  main_event_soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     30\u001b[0m                   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mstr\u001b[39m \n\u001b[0;32m     31\u001b[0m                   \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent-details\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m item\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\rautm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\bs4\\__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features))\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "get_urls.get_event_urls()\n",
    "# get_urls.get_fight_urls()\n",
    "# get_urls.get_fighter_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries for web-scraping and saving to CSV file.\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "#Creates a directory for scraped files if it doesn't already exist\n",
    "os.makedirs('scraped_files',exist_ok=True)\n",
    "\n",
    "#Define paths for url folder and scraped files folder\n",
    "url_path = os.getcwd() + '/urls'\n",
    "file_path = os.getcwd() + '/scraped_files'\n",
    "\n",
    "#Creates csv file for scraped data\n",
    "def create_csv_file():\n",
    "    \n",
    "    #If file does not exist, create a new CSV file with column headers\n",
    "    \n",
    "    if 'ufc_event_data.csv' not in os.listdir(file_path):\n",
    "        with open(file_path + '/' + 'ufc_event_data.csv','w', newline='',encoding='UTF8') as ufc_event_data:\n",
    "            writer = csv.writer(ufc_event_data)\n",
    "            writer.writerow(['event_name',\n",
    "                             'event_date',\n",
    "                             'event_city',\n",
    "                             'event_state',\n",
    "                             'event_country',\n",
    "                             'event_url'])\n",
    "        print('New File Created - ufc_event_data.csv')\n",
    "    else:\n",
    "        print('Scraping to Existing File - ufc_event_data.csv')\n",
    "\n",
    "#Ensure each url is only scraped once when script is run multiple times\n",
    "def filter_duplicate_urls(event_urls):\n",
    "    if 'ufc_event_data.csv' in os.listdir(file_path):\n",
    "        with open(file_path + '/' + 'ufc_event_data.csv','r') as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "            \n",
    "            #List of previously scraped urls:\n",
    "            \n",
    "            scraped_event_urls = [row['event_url'] for row in reader]\n",
    "            #Removes scraped urls from event_urls\n",
    "            for url in scraped_event_urls:\n",
    "                if url in event_urls:\n",
    "                    event_urls.remove(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\rautm\\\\Documents\\\\Notebooks\\\\UFC Project/urls'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrapes details of each UFC event appends to CSV file 'ufc_event_data'\n",
    "# def scrape_events():\n",
    "\n",
    "#Get event URLs from file 'event_urls.csv'\n",
    "if 'event_urls.csv' in os.listdir(url_path):\n",
    "    with open(url_path + '/' + 'event_urls.csv','r') as events_csv:\n",
    "        reader = csv.reader(events_csv)\n",
    "        event_urls = [row[0] for row in reader]\n",
    "else:\n",
    "    print(\"Missing file - event_urls.csv. Try running 'get_urls.get_event_urls()'\")\n",
    "\n",
    "#Removes urls that have been scraped already\n",
    "filter_duplicate_urls(event_urls)\n",
    "\n",
    "urls_to_scrape = len(event_urls)\n",
    "\n",
    "\n",
    "    # if urls_to_scrape == 0:\n",
    "    #     print('Event data already scraped')\n",
    "        \n",
    "    # else:\n",
    "    #     create_csv_file()\n",
    "\n",
    "    #     print(f'Scraping {urls_to_scrape} event URLs...')\n",
    "    #     urls_scraped = 0\n",
    "        \n",
    "    #     with open(file_path + '/' + 'ufc_event_data.csv','a+') as csv_file:\n",
    "    #         writer = csv.writer(csv_file)\n",
    "        \n",
    "    #         #Iterates through each event url to scrape key details\n",
    "    #         for event in event_urls:\n",
    "    #             event_request = requests.get(event)\n",
    "    #             event_soup = bs4.BeautifulSoup(event_request.text,'lxml')\n",
    "    #             event_full_location = event_soup.select('li')[4].text.split(':')[1].strip().split(',')\n",
    "\n",
    "    #             try:\n",
    "    #                 event_name = event_soup.select('h2')[0].text\n",
    "    #                 event_date = str(datetime.strptime(event_soup.select('li')[3].text.split(':')[-1].strip(), '%B %d, %Y'))\n",
    "    #                 event_city = event_full_location[0]\n",
    "    #                 event_country = event_full_location[-1]\n",
    "                    \n",
    "    #                 #Check event location contains state details\n",
    "    #                 if len(event_full_location)>2:\n",
    "    #                     event_state = event_full_location[1]\n",
    "    #                 else:\n",
    "    #                     event_state = 'NULL'\n",
    "    #                 urls_scraped += 1\n",
    "                        \n",
    "    #             except IndexError as e:\n",
    "    #                 print(f\"Error scraping event page: {event}\")\n",
    "    #                 print(f\"Error details: {e}\")\n",
    "                    \n",
    "\n",
    "    #             #Adds new row to csv file\n",
    "    #             writer.writerow([event_name.strip(), \n",
    "    #                              event_date[0:10], \n",
    "    #                              event_city.strip(), \n",
    "    #                              event_state.strip(), \n",
    "    #                              event_country.strip(), \n",
    "    #                              event])\n",
    "                \n",
    "    #         print(f'{urls_scraped}/{urls_to_scrape} events successfully scraped')\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_to_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
